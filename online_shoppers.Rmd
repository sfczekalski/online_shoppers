---
title: "Online shoppers analysis"
author: "Stanislaw Czekalski, Szymon Puszcz"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tinytex)
xfun::session_info('rmarkdown')
tinytex::tinytex_root()
library(ggplot2)
library(tidyverse)
library(dplyr)
library(scales)
library(tidyr)
```

In this document we are going to analyze online shoppers data.

It describes 12,330 user sessions browsing online shop. Dataset comes from UCI Machine Learning Repository. Observations are associated with labels indicating whenever current session ended up with consumer buying something or not. Primary task that can be done using this data is to perform classification of sessions, but the aim of our analysys is to explore the data and get knowledge about users' behaviour.

Raw data looks like this:
```{r read_data}
df = read.csv("online_shoppers_intention.csv")
head(df, 5)
```

We can see that variables of different types, some of them are categorical. We will set their types to factors.

```{r factors, echo=FALSE}
df$OperatingSystems <- factor(df$OperatingSystems)
df$Browser <- factor(df$Browser)
df$Region <- factor(df$Region)
df$TrafficType <- factor(df$TrafficType)
df$VisitorType <- factor(df$VisitorType)
df$Month <- factor(df$Month, levels = c("Feb", "Mar", "May", "June", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
```

We should always check for missing values, executing any(is.na(df)) comes in handy.
```{r missing_values, echo=FALSE}
any(is.na(df))
```

There is no missing values in this data.

First thing that must be done is checking class distribution. Most frequently, we do it using bar plot.
As one can see, we deal with a problem of imbalanced class distribution.

```{r class_distribution_relative, echo=FALSE, warning=FALSE}
ggplot(data = df, aes(x = Revenue, y = (..count..)/sum(..count..))) + 
  geom_bar(mapping = aes(fill = Revenue)) +
  geom_text(aes(label = scales::percent((..count..)/sum(..count..))), stat= "count", vjust = -.5) +
  scale_y_continuous(labels=scales::percent) +
  ylab("Class frequencies")+
    theme_light() 
```


Let's print summary of variables' distributions.
```{r distributions_summary, echo=FALSE, warning=FALSE}
summary(df)
```

We can see that most user visit parts of the website that are product related. They also spend the most time on them. "**Bounce Rate**", "**Exit Rate**" and "**Page Value**" are somehow misleading names, after looking up we may discover that they are related to Google Analytics names. 

"**Bounce Rate**" describe percentage of visitors that come from Google Analytics,  enter the site and then leave ("bounce") without triggering any other requests to the analytics server during that session.

"**Exit Rate**" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. 

The "**Page Value**" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. Values of all "Bounde Rate" and "Exit Rate" are quite low. To furhter investigate this features we can plot their distributions.

Most of values for all three of them are zeros, so we will only plot distribution of non zero values to have a closer look. Additionaly, we decided to split each distribution to two, depending on classes.

```{r BounceRates_distribution_classes, echo=FALSE, warning=FALSE}
df %>%
  filter(BounceRates > 0) %>%
  ggplot(mapping = aes(x = BounceRates, fill = Revenue, color = Revenue)) + 
  geom_density(alpha=.5)+
    theme_light() 
```

```{r ExitRates_distribution_classes, echo=FALSE, warning=FALSE}
df %>%
  filter(ExitRates > 0) %>%
  ggplot(mapping = aes(x = ExitRates, fill = Revenue, color = Revenue)) + 
  geom_density(alpha=.5) +
  theme_light()
```

```{r PageValues_distribution_classes, echo=FALSE, warning=FALSE}
df %>%
  filter(PageValues > 0) %>%
  ggplot(mapping = aes(x = PageValues, fill = Revenue, color = Revenue)) + 
  geom_density(alpha=.5) +
  theme_light()
```

We can see that distributions of all three features differ depending on classes. Especially, all observations having bounce rates and exit rates 0.20 belong to negative class. Distribution of page values is more skewed towards high values for positive class. It means that all three variables might be usefull for building classification model.

Next, we will take a look at times the user spend on different parts of the website.

```{r times_histograms, echo=FALSE, warning=FALSE}
df %>%
  filter(Administrative > 0) %>%
  ggplot(mapping = aes(x = Administrative, fill = Revenue, color = Revenue)) + 
  geom_density(alpha=.5) +
  theme_light()

df %>%
  filter(Informational > 0) %>%
  ggplot(mapping = aes(x = Informational, fill = Revenue, color = Revenue)) + 
  geom_density(alpha=.5) +
  theme_light()

df %>%
  filter(ProductRelated > 0) %>%
  ggplot(mapping = aes(x = ProductRelated, fill = Revenue, color = Revenue)) + 
  geom_density(alpha=.5) +
  theme_light()
```

As one can see, there is no much difference bitween times spend on parts of website depending on class. This raw features might not be useful for prediction.

```{r month, echo=FALSE, warning=FALSE}
# Class proportion depending on month
ggplot(data = df, aes(x = Revenue, y = (..count..)/sum(..count..)), group = Month) + 
  geom_bar(mapping = aes(fill = Revenue)) +
  geom_text(aes(label = scales::percent((..count..)/sum(..count..))), stat= "count", vjust = -.5) +
  scale_y_continuous(labels=scales::percent) +
  facet_wrap(~Month, ncol = 3) +
  ylab("Relative class frequencies") +
  theme_light()
```

Let's look at month information. Data is not evenly distributed between months, for some of them we have very little information. For instance, classification model might learn, that there is no point predicting a purchase for session in June, because chances are that no session ending with a purchase from this month will be part of our training set. Using this feature might cause overfitting.

```{r os, echo=FALSE, warning=FALSE}
# Operating system
ggplot(data = df) + 
  geom_bar(mapping = aes(x = OperatingSystems, fill = OperatingSystems)) +
  ylab("Count") +
  theme_light()

df %>%
  filter(OperatingSystems == 1:4) %>%
  ggplot() + 
  geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
  facet_wrap(~OperatingSystems) +
  ylab("Class count depending on OS") +
  theme_light()

df %>%
  filter(OperatingSystems == 5:8) %>%
  ggplot() + 
  geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
  facet_wrap(~OperatingSystems) +
  ylab("Class count depending on OS: rare categories") +
  theme_light()

```

There is 8 kinds of operating systems used by our users. Some of them are much more popular, and some of them are very rare. It may make sense to group all rare categories into one, because alone they are not very informative. We may wrongly conclude that people using OS number 5 buy products more often than other users, but we need to be careful, that little data can be misleading.

```{r browsers, echo=FALSE, warning=FALSE}
# Browser
ggplot(data = df) + 
  geom_bar(mapping = aes(x = Browser, fill = Browser)) +
  ylab("Count") +
  theme_light()

# Class count depending on browser
df %>%
  filter(Browser == 1 | Browser == 2) %>%
  ggplot() + 
    geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
    facet_wrap(~Browser) +
    ylab("Class count depending on browser") +
  theme_light()

df %>%
  filter(Browser == 3:13) %>%
  ggplot() + 
  geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
  facet_wrap(~Browser) +
  ylab("Class count depending on browser") +
  theme_light()
```

In case of browsers, we have two main categories, and quite a few less popular ones. We draw class distribution in each category to check if we may infer some class information depending on browser type. Once again, grouping rare categories might be desirable

```{r region, echo=FALSE, warning=FALSE}
# Region
ggplot(data = df) + 
  geom_bar(mapping = aes(x = Region, fill = Region)) +
  ylab("Count") +
  theme_light()

# Class count in each region
ggplot(data = df) + 
  geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
  facet_wrap(~Region) +
  ylab("Class count in each region") +
  theme_light()
```

Region distribution is more even compared to OS and browser. Still, class distribution in each region category doesn't seem to be very helpful in classification.


```{r traffic_type, echo=FALSE, warning=FALSE}
# Traffic type
ggplot(data = df) + 
  geom_bar(mapping = aes(x = TrafficType, fill = TrafficType)) +
  ylab("Count") +
  theme_light()

# Class depending on traffic type
ggplot(data = df) + 
  geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
  facet_wrap(~TrafficType, nrow = 4) +
  ylab("Class proportion") +
  theme_light()
```

Traffic type distribution is very uneven. This variable might be interesting, especially rare categories might represent untypical user, i.e. administrator or developer. We take a closer look below.

```{r traffic_type_closer, echo=FALSE, warning=FALSE}
# What about some traffic types? Maybe they describe user that are no customers
df %>%
  filter(TrafficType==7 | TrafficType==9 | TrafficType==11 | TrafficType==13 | TrafficType == 19) %>%
  ggplot() +
    geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
    facet_wrap(~TrafficType, nrow = 4) +
    ylab("Class proportion") +
  theme_light()

df %>%
  filter(TrafficType==14 | TrafficType==15 | TrafficType==16 | TrafficType==17 | TrafficType == 18) %>%
  ggplot() +
  geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
  facet_wrap(~TrafficType, nrow = 4) +
  ylab("Class proportion") +
  theme_light()

```

It is hard to reason what some traffic types might represent due to values annonymization. If we had access to description of individual levels of this feature we could discover that some traffic types come from people that are not typical users, and we are not interested in their behaviour. In this case, such observations should be removed from the dataset.

```{r visitor_type, echo=FALSE, warning=FALSE}
# Visitor type
ggplot(data = df) + 
  geom_bar(mapping = aes(x = VisitorType, fill = VisitorType)) +
  ylab("Count") +
  theme_light()

# Class depending on visitor type
ggplot(data = df) + 
  geom_bar(mapping = aes(x = Revenue, fill = Revenue)) +
  facet_grid(~VisitorType) +
  ylab("Class proportion") +
  theme_light()

```

Most of the visitors are returning ones. Category "Other" looks suspicious, in this case we can be pretty sure it represents some abnormal type of users, like administrator or Google crowler, that is not likely to buy something!

```{r weekend, echo=FALSE, warning=FALSE}
# Weekend / no weekend count
ggplot(data = df) + 
  geom_bar(mapping = aes(x = Weekend, fill = Weekend)) +
  ylab("Weekend count") +
  theme_light()

# Class depending on weekend / no weekend

ggplot(data = df %>% filter(Weekend==FALSE), 
       aes(x = Revenue, y = (..count..)/sum(..count..))
       ) + 
  geom_bar(mapping = aes(fill = Revenue)) +
  geom_text(aes(label = scales::percent((..count..)/sum(..count..))), stat= "count", vjust = -.5) +
  scale_y_continuous(labels=scales::percent) +
  # facet_grid(~Weekend) +
  ylab("Relative class frequencies") +
  ggtitle("Revenue distribution during workdays") +
  theme_light()

ggplot(data = df %>% filter(Weekend==TRUE), 
       aes(x = Revenue, y = (..count..)/sum(..count..))
       ) + 
  geom_bar(mapping = aes(fill = Revenue)) +
  geom_text(aes(label = scales::percent((..count..)/sum(..count..))), stat= "count", vjust = -.5) +
  scale_y_continuous(labels=scales::percent) +
  # facet_grid(~Weekend) +
  ylab("Relative class frequencies") + 
  ggtitle("Revenue distribution during weekend") +
  theme_light()
```

We can see that users that came to the website during the weekend are a little bit more likely to buy something. During the week rate of buyers is about 17.5%, and for the weekend its 20%. It is not a suprise, during the weekend people usually have more time for shopping.

## Random Forest proximity
In order to get a better understanding of our data, we may try to capture its structure. As we've observed our data consists of variables of different types: we have both numeric and categorical features, so it is difficult to come up with a metric suitable for clustering. What we can do instead is to train Random Forest, and get pairwise similarities between objects by counting how many times objects end up in the same leaf node, and use this proximity matrix to perform Multidimensional Scalling, that would create two-dimensional embediing of our data in some abstract space.

```{r rf, echo=FALSE}
library(randomForest)

# this method is very memory intensive - it requires O(n**2) space
# because of this, we use only 30% fraction of our data that can fit in RAM
# stratified split is performed
set.seed(42)
sample_df <- df %>%
  group_by(Revenue) %>%
  sample_frac(0.2) %>%
  ungroup

set.seed(42)
rf <- randomForest(x = sample_df[,1:17], y = factor(sample_df$Revenue), ntree = 100, proximity = T, 
                   importance = TRUE, do.trace = 100)
prox <- rf$proximity

print(rf)
```

OOB estimate of error suggests that we can trust the model.

## Multidimensional Scalling

```{r mds, echo=FALSE}
library(MVA)

set.seed(42)
mds <- cmdscale(1/(prox +1))


# emulate ggplot color palette
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

cols = gg_color_hue(2)

mds_df <- data.frame(mds)
mds_df$class <- sample_df$Revenue

mds_df %>%
  ggplot(mapping = aes(x = mds_df[,1], y = mds_df[,2], color = class)) + 
  geom_point(alpha=.5) +
  theme_light()
```

Some structure of our data was obtained. We can see dense cluster of sessions that finished whith purchase, and a whole bunch of other sessions.

Let's check which features our model considers important.

```{r rf_importances}
importance(rf)
varImpPlot(rf)

```

Page Values seem to be really important, along with time spend on product-related page, laso exit rate and some other features. Month information might be a little bit confusing - our data is not very representative when it comes to this feature.

We will try Multidimensional Scalling on proximity from Random Forest trained only on most important features.


```{r mds_important}
important_cols <- c("PageValues", "ProductRelated_Duration", "ExitRates", "ProductRelated",
                    "Month", "BounceRates", "Administrative_Duration", "TrafficType", 
                    "Region", "Administrative")

set.seed(42)
rf_imp <- randomForest(x = sample_df[,important_cols], y = factor(sample_df$Revenue), ntree = 100, proximity = T)
prox_imp <- rf_imp$proximity

set.seed(42)
mds_imp <- cmdscale(1/(prox_imp +1))

mds_df <- data.frame(mds_imp)
mds_df$class <- sample_df$Revenue

mds_df %>%
  ggplot(mapping = aes(x = mds_df[,1], y = mds_df[,2], color = class)) + 
  geom_point(alpha=.5) +
  theme_light()
```

The space is filpped horizontally, but overal structure remains similar. We can observe that in the case of using only most important features, more separeted clusters are visible.

## Umap
For some experimentation, we can try using UMAP algorithm on proximity matrix produced by Random Forest.
It produces denser clusters, small one at the bottom left consists mostly of schoppers session! 


```{r umap}
library(umap)
library(reticulate)

dist <- 1/(1+prox)
umap.res <- umap(dist, method = "umap-learn", n_components=2, metric="precomputed", transform_seed=42, 
                 n_neighbors=15, min_dist=0.2)
embedding <- umap.res$layout

embedding_df <- data.frame(embedding)
embedding_df$class <- sample_df$Revenue

embedding_df %>%
  ggplot(mapping = aes(x = embedding_df[,1], y = embedding_df[,2], color = class)) + 
  geom_point(alpha=.5) +
  theme_light()

```